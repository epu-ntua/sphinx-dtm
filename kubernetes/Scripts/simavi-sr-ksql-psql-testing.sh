
# Enable cleanup.policy=compact for Schema Registry to work

KAFKA_PATH=$("C:\Tools\Kafka\kafka_2.12-2.5.0\bin")
cd $KAFKA_PATH

./kafka-configs.sh --bootstrap-server Sphinx:9092 --entity-type topics --entity-name _schemas --alter --add-config cleanup.policy=compact




# After it's done, we set up KSQL STREAMS and CONNECTORS for writing alerts in PostgreSQL

ksqldbserverpod=$(kubectl get pods -o=name --selector=channels=ksql)
kubectl exec --stdin --tty $ksqldbserverpod -- ksql http://ksqldb-server:8088 -c "help"

SET 'auto.offset.reset'='earliest';



CREATE STREAM AD_JDBC_SOURCE_JSON 
	(ID INT KEY, DESCRIPTION VARCHAR, TIMESTAMP VARCHAR, INDICATION VARCHAR, ACTION VARCHAR, DETAILS VARCHAR) \
        WITH (KAFKA_TOPIC='jdbc_table_source_ad', VALUE_FORMAT='JSON', PARTITIONS=1);


CREATE STREAM DTM_JDBC_SOURCE_JSON 
	(ID INT KEY, DESCRIPTION VARCHAR, TIMESTAMP VARCHAR, INDICATION VARCHAR, ACTION VARCHAR, DETAILS VARCHAR) \
        WITH (KAFKA_TOPIC='jdbc_table_source_dtm', VALUE_FORMAT='JSON', PARTITIONS=1);



CREATE STREAM AE_JDBC_SOURCE_JSON 
	(ID INT KEY, DESCRIPTION VARCHAR, TIMESTAMP VARCHAR, INDICATION VARCHAR, ACTION VARCHAR, DETAILS VARCHAR) \
        WITH (KAFKA_TOPIC='jdbc_table_source_ae', VALUE_FORMAT='JSON', PARTITIONS=1);



CREATE STREAM BBTR_JDBC_SOURCE_JSON 
	(ID INT KEY, DESCRIPTION VARCHAR, TIMESTAMP VARCHAR, INDICATION VARCHAR, ACTION VARCHAR, DETAILS VARCHAR) \
        WITH (KAFKA_TOPIC='jdbc_table_source_bbtr', VALUE_FORMAT='JSON', PARTITIONS=1);



CREATE STREAM HP_JDBC_SOURCE_JSON 
	(ID INT KEY, DESCRIPTION VARCHAR, TIMESTAMP VARCHAR, INDICATION VARCHAR, ACTION VARCHAR, DETAILS VARCHAR) \
        WITH (KAFKA_TOPIC='jdbc_table_source_hp', VALUE_FORMAT='JSON', PARTITIONS=1);


CREATE STREAM RCRA_JDBC_SOURCE_JSON 
	(ID INT KEY, DESCRIPTION VARCHAR, TIMESTAMP VARCHAR, INDICATION VARCHAR, ACTION VARCHAR, DETAILS VARCHAR) \
        WITH (KAFKA_TOPIC='jdbc_table_source_rcra', VALUE_FORMAT='JSON', PARTITIONS=1);


CREATE STREAM KB_JDBC_SOURCE_JSON 
	(ID INT KEY, DESCRIPTION VARCHAR, TIMESTAMP VARCHAR, INDICATION VARCHAR, ACTION VARCHAR, DETAILS VARCHAR) \
        WITH (KAFKA_TOPIC='jdbc_table_source_kb', VALUE_FORMAT='JSON', PARTITIONS=1);


CREATE STREAM DSS_JDBC_SOURCE_JSON 
	(ID INT KEY, DESCRIPTION VARCHAR, TIMESTAMP VARCHAR, INDICATION VARCHAR, ACTION VARCHAR, DETAILS VARCHAR) \
        WITH (KAFKA_TOPIC='jdbc_table_source_dss', VALUE_FORMAT='JSON', PARTITIONS=1);


CREATE STREAM VAAAS_JDBC_SOURCE_JSON 
	(ID INT KEY, DESCRIPTION VARCHAR, TIMESTAMP VARCHAR, INDICATION VARCHAR, ACTION VARCHAR, DETAILS VARCHAR) \
        WITH (KAFKA_TOPIC='jdbc_table_source_vaaas', VALUE_FORMAT='JSON', PARTITIONS=1);



CREATE STREAM SIEM_JDBC_SOURCE_JSON 
	(ID INT KEY, DESCRIPTION VARCHAR, TIMESTAMP VARCHAR, INDICATION VARCHAR, ACTION VARCHAR, DETAILS VARCHAR) \
        WITH (KAFKA_TOPIC='jdbc_table_source_siem', VALUE_FORMAT='JSON', PARTITIONS=1);




CREATE STREAM JDBC_SOURCE_AVRO 
	(ID INT KEY, DESCRIPTION VARCHAR, TIMESTAMP VARCHAR, INDICATION VARCHAR, TOOL VARCHAR, STATUS VARCHAR, ACTION VARCHAR, DETAILS VARCHAR, SENT VARCHAR) \
        WITH (KAFKA_TOPIC='JDBC_SOURCE_AVRO', PARTITIONS=1, VALUE_FORMAT='AVRO');


INSERT INTO JDBC_SOURCE_AVRO SELECT ID, DESCRIPTION, TIMESTAMP, INDICATION, 'AD' AS TOOL, '--' AS STATUS, ACTION, DETAILS, 'false' AS SENT FROM AD_JDBC_SOURCE_JSON;

INSERT INTO JDBC_SOURCE_AVRO SELECT ID, DESCRIPTION, TIMESTAMP, INDICATION, 'DTM' AS TOOL, '--' AS STATUS,  ACTION, DETAILS, 'false' AS SENT FROM DTM_JDBC_SOURCE_JSON;

INSERT INTO JDBC_SOURCE_AVRO SELECT ID, DESCRIPTION, TIMESTAMP, INDICATION, 'AE' AS TOOL, '--' AS STATUS,  ACTION, DETAILS, 'false' AS SENT FROM AE_JDBC_SOURCE_JSON;

INSERT INTO JDBC_SOURCE_AVRO SELECT ID, DESCRIPTION, TIMESTAMP, INDICATION, 'BBTR' AS TOOL, '--' AS STATUS,  ACTION, DETAILS, 'false' AS SENT FROM BBTR_JDBC_SOURCE_JSON;

INSERT INTO JDBC_SOURCE_AVRO SELECT ID, DESCRIPTION, TIMESTAMP, INDICATION, 'HP' AS TOOL, '--' AS STATUS,  ACTION, DETAILS, 'false' AS SENT FROM HP_JDBC_SOURCE_JSON;

INSERT INTO JDBC_SOURCE_AVRO SELECT ID, DESCRIPTION, TIMESTAMP, INDICATION, 'RCRA' AS TOOL, '--' AS STATUS,  ACTION, DETAILS, 'false' AS SENT FROM RCRA_JDBC_SOURCE_JSON;

INSERT INTO JDBC_SOURCE_AVRO SELECT ID, DESCRIPTION, TIMESTAMP, INDICATION, 'KB' AS TOOL, '--' AS STATUS,  ACTION, DETAILS, 'false' AS SENT FROM KB_JDBC_SOURCE_JSON;

INSERT INTO JDBC_SOURCE_AVRO SELECT ID, DESCRIPTION, TIMESTAMP, INDICATION, 'DSS' AS TOOL, '--' AS STATUS,  ACTION, DETAILS, 'false' AS SENT FROM DSS_JDBC_SOURCE_JSON;

INSERT INTO JDBC_SOURCE_AVRO SELECT ID, DESCRIPTION, TIMESTAMP, INDICATION, 'VAAAS' AS TOOL, '--' AS STATUS,  ACTION, DETAILS, 'false' AS SENT FROM VAAAS_JDBC_SOURCE_JSON;

INSERT INTO JDBC_SOURCE_AVRO SELECT ID, DESCRIPTION, TIMESTAMP, INDICATION, 'SIEM' AS TOOL, '--' AS STATUS,  ACTION, DETAILS, 'false' AS SENT FROM SIEM_JDBC_SOURCE_JSON;



CREATE SINK CONNECTOR jdbc_dest_4 WITH (
  'connector.class'          = 'io.confluent.connect.jdbc.JdbcSinkConnector',
  'connection.url'           = 'jdbc:postgresql://sphinx-postgres:5432/sphinx',
  'connection.user'          = 'sphinx',
  'connection.password'      = 'sphinx',
  'tasks.max'		     = '10',
  'mode'                     = 'incrementing',
  'key'                      = 'id',
  'key.converter'            = 'org.apache.kafka.connect.converters.IntegerConverter',
  'value.converter'          = 'io.confluent.connect.avro.AvroConverter',
  'value.converter.schema.registry.url'= 'http://schema-registry-service:8081',
  'key.converter.schemas.enable' = false,
  'value.converter.schemas.enable' = true,
  'pk.mode'                  = 'none',
  'topics'                    = 'JDBC_SOURCE_AVRO',
  'table.name.format'        = 'kafka_${topic}',
  'auto.create'              = true
);

exit;

postgresdbpod=$(kubectl get pods -o=name --selector=tier=simavidb)
kubectl exec --stdin $postgresdbpod -- psql -h localhost -U sphinx -p 5432 sphinx -c "SELECT * FROM \"kafka_JDBC_SOURCE_AVRO\""